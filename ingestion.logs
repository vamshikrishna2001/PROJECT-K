:: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-817cda07-ec4a-44ba-9e20-bdf209022ed5;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central
	found org.apache.kafka#kafka-clients;2.8.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.8.4 in central
	found org.slf4j#slf4j-api;1.7.32 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.2 in central
	found org.spark-project.spark#unused;1.0.0 in central
	found org.apache.hadoop#hadoop-client-api;3.3.2 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 1456ms :: artifacts dl 53ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]
	org.apache.kafka#kafka-clients;2.8.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;1.7.32 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.8.4 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-817cda07-ec4a-44ba-9e20-bdf209022ed5
	confs: [default]
	0 artifacts copied, 12 already retrieved (0kB/16ms)
24/07/20 16:04:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
creating spark job
24/07/20 16:04:56 INFO SparkContext: Running Spark version 3.5.1
24/07/20 16:04:56 INFO SparkContext: OS info Linux, 6.1.85+, amd64
24/07/20 16:04:56 INFO SparkContext: Java version 17.0.11
24/07/20 16:04:56 INFO ResourceUtils: ==============================================================
24/07/20 16:04:56 INFO ResourceUtils: No custom resources configured for spark.driver.
24/07/20 16:04:56 INFO ResourceUtils: ==============================================================
24/07/20 16:04:56 INFO SparkContext: Submitted application: KafkaSparkConsumer
24/07/20 16:04:56 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/07/20 16:04:56 INFO ResourceProfile: Limiting resource is cpu
24/07/20 16:04:56 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/07/20 16:04:56 INFO SecurityManager: Changing view acls to: root,spark
24/07/20 16:04:56 INFO SecurityManager: Changing modify acls to: root,spark
24/07/20 16:04:56 INFO SecurityManager: Changing view acls groups to: 
24/07/20 16:04:56 INFO SecurityManager: Changing modify acls groups to: 
24/07/20 16:04:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
24/07/20 16:04:56 INFO Utils: Successfully started service 'sparkDriver' on port 45171.
24/07/20 16:04:56 INFO SparkEnv: Registering MapOutputTracker
24/07/20 16:04:56 INFO SparkEnv: Registering BlockManagerMaster
24/07/20 16:04:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/07/20 16:04:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/07/20 16:04:56 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/07/20 16:04:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9514d113-5ba9-4d8b-bde2-a74be38ba3ae
24/07/20 16:04:56 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/07/20 16:04:57 INFO SparkEnv: Registering OutputCommitCoordinator
24/07/20 16:04:57 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/07/20 16:04:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/07/20 16:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at spark://f29f783329e2:45171/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at spark://f29f783329e2:45171/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at spark://f29f783329e2:45171/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://f29f783329e2:45171/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://f29f783329e2:45171/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://f29f783329e2:45171/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at spark://f29f783329e2:45171/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://f29f783329e2:45171/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at spark://f29f783329e2:45171/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at spark://f29f783329e2:45171/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at spark://f29f783329e2:45171/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://f29f783329e2:45171/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at spark://f29f783329e2:45171/files/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-c343c4c1-c761-4a36-9bcf-923a1c1be07e/userFiles-5f52aacc-3342-4133-9f00-4ca1180f638c/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
24/07/20 16:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at spark://f29f783329e2:45171/files/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-c343c4c1-c761-4a36-9bcf-923a1c1be07e/userFiles-5f52aacc-3342-4133-9f00-4ca1180f638c/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
24/07/20 16:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at spark://f29f783329e2:45171/files/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-c343c4c1-c761-4a36-9bcf-923a1c1be07e/userFiles-5f52aacc-3342-4133-9f00-4ca1180f638c/org.apache.kafka_kafka-clients-2.8.1.jar
24/07/20 16:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://f29f783329e2:45171/files/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-c343c4c1-c761-4a36-9bcf-923a1c1be07e/userFiles-5f52aacc-3342-4133-9f00-4ca1180f638c/com.google.code.findbugs_jsr305-3.0.0.jar
24/07/20 16:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://f29f783329e2:45171/files/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-c343c4c1-c761-4a36-9bcf-923a1c1be07e/userFiles-5f52aacc-3342-4133-9f00-4ca1180f638c/org.apache.commons_commons-pool2-2.11.1.jar
24/07/20 16:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://f29f783329e2:45171/files/org.spark-project.spark_unused-1.0.0.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-c343c4c1-c761-4a36-9bcf-923a1c1be07e/userFiles-5f52aacc-3342-4133-9f00-4ca1180f638c/org.spark-project.spark_unused-1.0.0.jar
24/07/20 16:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at spark://f29f783329e2:45171/files/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-c343c4c1-c761-4a36-9bcf-923a1c1be07e/userFiles-5f52aacc-3342-4133-9f00-4ca1180f638c/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
24/07/20 16:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://f29f783329e2:45171/files/org.lz4_lz4-java-1.8.0.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-c343c4c1-c761-4a36-9bcf-923a1c1be07e/userFiles-5f52aacc-3342-4133-9f00-4ca1180f638c/org.lz4_lz4-java-1.8.0.jar
24/07/20 16:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at spark://f29f783329e2:45171/files/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-c343c4c1-c761-4a36-9bcf-923a1c1be07e/userFiles-5f52aacc-3342-4133-9f00-4ca1180f638c/org.xerial.snappy_snappy-java-1.1.8.4.jar
24/07/20 16:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at spark://f29f783329e2:45171/files/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-c343c4c1-c761-4a36-9bcf-923a1c1be07e/userFiles-5f52aacc-3342-4133-9f00-4ca1180f638c/org.slf4j_slf4j-api-1.7.32.jar
24/07/20 16:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at spark://f29f783329e2:45171/files/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-c343c4c1-c761-4a36-9bcf-923a1c1be07e/userFiles-5f52aacc-3342-4133-9f00-4ca1180f638c/org.apache.hadoop_hadoop-client-api-3.3.2.jar
24/07/20 16:04:57 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://f29f783329e2:45171/files/commons-logging_commons-logging-1.1.3.jar with timestamp 1721491496096
24/07/20 16:04:57 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-c343c4c1-c761-4a36-9bcf-923a1c1be07e/userFiles-5f52aacc-3342-4133-9f00-4ca1180f638c/commons-logging_commons-logging-1.1.3.jar
24/07/20 16:04:58 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
24/07/20 16:04:58 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.4:7077 after 53 ms (0 ms spent in bootstraps)
24/07/20 16:04:58 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240720160458-0001
24/07/20 16:04:58 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240720160458-0001/0 on worker-20240720154058-172.18.0.9-39069 (172.18.0.9:39069) with 1 core(s)
24/07/20 16:04:58 INFO StandaloneSchedulerBackend: Granted executor ID app-20240720160458-0001/0 on hostPort 172.18.0.9:39069 with 1 core(s), 1024.0 MiB RAM
24/07/20 16:04:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38415.
24/07/20 16:04:58 INFO NettyBlockTransferService: Server created on f29f783329e2:38415
24/07/20 16:04:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/07/20 16:04:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, f29f783329e2, 38415, None)
24/07/20 16:04:58 INFO BlockManagerMasterEndpoint: Registering block manager f29f783329e2:38415 with 434.4 MiB RAM, BlockManagerId(driver, f29f783329e2, 38415, None)
24/07/20 16:04:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, f29f783329e2, 38415, None)
24/07/20 16:04:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, f29f783329e2, 38415, None)
24/07/20 16:04:58 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240720160458-0001/0 is now RUNNING
24/07/20 16:04:59 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
after creating spark job
24/07/20 16:05:00 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/07/20 16:05:00 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
24/07/20 16:05:08 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.9:46036) with ID 0,  ResourceProfileId 0
24/07/20 16:05:08 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.9:36951 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.9, 36951, None)
24/07/20 16:05:10 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/07/20 16:05:10 INFO ResolveWriteToStream: Checkpoint root hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka resolved to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka.
24/07/20 16:05:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/07/20 16:05:11 INFO MicroBatchExecution: Starting [id = 4e439331-2ef5-4ccd-807b-7735ec4fe321, runId = 300a8ed0-c967-4502-864f-9b6811adc4ae]. Use hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka to store the query checkpoint.
24/07/20 16:05:11 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@143a90dc] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@1f29175]
24/07/20 16:05:11 INFO OffsetSeqLog: BatchIds found from listing: 
24/07/20 16:05:11 INFO OffsetSeqLog: BatchIds found from listing: 
24/07/20 16:05:11 INFO MicroBatchExecution: Starting new streaming query.
24/07/20 16:05:11 INFO MicroBatchExecution: Stream started from {}
24/07/20 16:05:12 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [kafka:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/07/20 16:05:13 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.
24/07/20 16:05:13 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.
24/07/20 16:05:13 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.
24/07/20 16:05:13 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.
24/07/20 16:05:13 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.
24/07/20 16:05:13 INFO AppInfoParser: Kafka version: 2.8.1
24/07/20 16:05:13 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
24/07/20 16:05:13 INFO AppInfoParser: Kafka startTimeMs: 1721491513076
24/07/20 16:05:13 INFO CheckpointFileManager: Writing atomically to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/sources/0/0 using temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/sources/0/.0.a77748e6-f2b3-4993-911f-f4d1f3cef154.tmp
24/07/20 16:05:13 INFO CheckpointFileManager: Renamed temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/sources/0/.0.a77748e6-f2b3-4993-911f-f4d1f3cef154.tmp to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/sources/0/0
24/07/20 16:05:13 INFO KafkaMicroBatchStream: Initial offsets: {"classifiedMessages":{"0":0}}
24/07/20 16:05:14 INFO CheckpointFileManager: Writing atomically to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/0 using temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/.0.aa7c1ed1-07d6-4f24-a924-3b0ab9d3b66a.tmp
24/07/20 16:05:14 INFO CheckpointFileManager: Renamed temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/.0.aa7c1ed1-07d6-4f24-a924-3b0ab9d3b66a.tmp to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/0
24/07/20 16:05:14 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1721491513990,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/07/20 16:05:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 16:05:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 16:05:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 16:05:15 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 16:05:15 INFO FileStreamSinkLog: BatchIds found from listing: 
24/07/20 16:05:15 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
24/07/20 16:05:16 INFO CodeGenerator: Code generated in 417.147797 ms
24/07/20 16:05:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/07/20 16:05:16 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/07/20 16:05:16 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/07/20 16:05:16 INFO DAGScheduler: Parents of final stage: List()
24/07/20 16:05:16 INFO DAGScheduler: Missing parents: List()
24/07/20 16:05:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/07/20 16:05:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 232.7 KiB, free 434.2 MiB)
24/07/20 16:05:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 83.9 KiB, free 434.1 MiB)
24/07/20 16:05:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on f29f783329e2:38415 (size: 83.9 KiB, free: 434.3 MiB)
24/07/20 16:05:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/07/20 16:05:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/07/20 16:05:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/07/20 16:05:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 10918 bytes) 
24/07/20 16:05:17 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.9:36951 (size: 83.9 KiB, free: 434.3 MiB)
24/07/20 16:05:23 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 7146 ms on 172.18.0.9 (executor 0) (1/1)
24/07/20 16:05:23 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/07/20 16:05:23 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 7.579 s
24/07/20 16:05:23 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/07/20 16:05:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/07/20 16:05:24 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 7.699878 s
24/07/20 16:05:24 INFO FileFormatWriter: Start to commit write Job 72aa522d-8da7-42e3-9c96-c936b05a8ecf.
24/07/20 16:05:24 INFO FileStreamSinkLog: Set the compact interval to 10 [defaultCompactInterval: 10]
24/07/20 16:05:24 INFO CheckpointFileManager: Writing atomically to hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/0 using temp file hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/.0.7130eb83-6bf8-4583-a969-3994008c566f.tmp
24/07/20 16:05:24 INFO CheckpointFileManager: Renamed temp file hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/.0.7130eb83-6bf8-4583-a969-3994008c566f.tmp to hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/0
24/07/20 16:05:24 INFO ManifestFileCommitProtocol: Committed batch 0
24/07/20 16:05:24 INFO FileFormatWriter: Write Job 72aa522d-8da7-42e3-9c96-c936b05a8ecf committed. Elapsed time: 68 ms.
24/07/20 16:05:24 INFO FileFormatWriter: Finished processing stats for write job 72aa522d-8da7-42e3-9c96-c936b05a8ecf.
24/07/20 16:05:24 INFO CheckpointFileManager: Writing atomically to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/0 using temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/.0.3403fa81-65cb-42e0-8cc4-db8ae688c444.tmp
24/07/20 16:05:24 INFO CheckpointFileManager: Renamed temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/.0.3403fa81-65cb-42e0-8cc4-db8ae688c444.tmp to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/0
24/07/20 16:05:24 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "4e439331-2ef5-4ccd-807b-7735ec4fe321",
  "runId" : "300a8ed0-c967-4502-864f-9b6811adc4ae",
  "name" : null,
  "timestamp" : "2024-07-20T16:05:11.637Z",
  "batchId" : 0,
  "numInputRows" : 30,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 2.3894862604540026,
  "durationMs" : {
    "addBatch" : 8977,
    "commitOffsets" : 67,
    "getBatch" : 31,
    "latestOffset" : 2230,
    "queryPlanning" : 1031,
    "triggerExecution" : 12544,
    "walCommit" : 67
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[classifiedMessages]]",
    "startOffset" : null,
    "endOffset" : {
      "classifiedMessages" : {
        "0" : 30
      }
    },
    "latestOffset" : {
      "classifiedMessages" : {
        "0" : 30
      }
    },
    "numInputRows" : 30,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 2.3894862604540026,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "FileSink[hdfs://hadoop-namenode:8020/user/spark/kafka-data]",
    "numOutputRows" : -1
  }
}
24/07/20 16:05:33 INFO BlockManagerInfo: Removed broadcast_0_piece0 on f29f783329e2:38415 in memory (size: 83.9 KiB, free: 434.4 MiB)
24/07/20 16:05:33 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.9:36951 in memory (size: 83.9 KiB, free: 434.4 MiB)
24/07/20 16:05:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:05:44 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:05:54 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:06:04 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:06:14 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:06:24 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:06:34 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:06:40 INFO CheckpointFileManager: Writing atomically to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/1 using temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/.1.6d9da5d1-e417-478f-8828-778340c2898c.tmp
24/07/20 16:06:40 INFO CheckpointFileManager: Renamed temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/.1.6d9da5d1-e417-478f-8828-778340c2898c.tmp to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/1
24/07/20 16:06:40 INFO MicroBatchExecution: Committed offsets for batch 1. Metadata OffsetSeqMetadata(0,1721491600148,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/07/20 16:06:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 16:06:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 16:06:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 16:06:40 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 16:06:40 INFO FileStreamSinkLog: BatchIds found from listing: 0, 0
24/07/20 16:06:40 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
24/07/20 16:06:40 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/07/20 16:06:40 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/07/20 16:06:40 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/07/20 16:06:40 INFO DAGScheduler: Parents of final stage: List()
24/07/20 16:06:40 INFO DAGScheduler: Missing parents: List()
24/07/20 16:06:40 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/07/20 16:06:40 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 232.7 KiB, free 434.2 MiB)
24/07/20 16:06:40 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 83.9 KiB, free 434.1 MiB)
24/07/20 16:06:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on f29f783329e2:38415 (size: 83.9 KiB, free: 434.3 MiB)
24/07/20 16:06:40 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/07/20 16:06:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/07/20 16:06:40 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/07/20 16:06:40 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.9, executor 0, partition 0, PROCESS_LOCAL, 10918 bytes) 
24/07/20 16:06:40 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.9:36951 (size: 83.9 KiB, free: 434.3 MiB)
24/07/20 16:06:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1392 ms on 172.18.0.9 (executor 0) (1/1)
24/07/20 16:06:41 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 1.462 s
24/07/20 16:06:41 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/07/20 16:06:41 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/07/20 16:06:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/07/20 16:06:41 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 1.475549 s
24/07/20 16:06:41 INFO FileFormatWriter: Start to commit write Job 7d2d029b-77ee-43a9-872c-a2f633d5d7ac.
24/07/20 16:06:41 INFO CheckpointFileManager: Writing atomically to hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/1 using temp file hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/.1.39969804-6905-4c6f-9439-4bc7a3c29790.tmp
24/07/20 16:06:41 INFO CheckpointFileManager: Renamed temp file hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/.1.39969804-6905-4c6f-9439-4bc7a3c29790.tmp to hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/1
24/07/20 16:06:41 INFO ManifestFileCommitProtocol: Committed batch 1
24/07/20 16:06:41 INFO FileFormatWriter: Write Job 7d2d029b-77ee-43a9-872c-a2f633d5d7ac committed. Elapsed time: 30 ms.
24/07/20 16:06:41 INFO FileFormatWriter: Finished processing stats for write job 7d2d029b-77ee-43a9-872c-a2f633d5d7ac.
24/07/20 16:06:41 INFO CheckpointFileManager: Writing atomically to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/1 using temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/.1.31b97e12-afed-42bc-8d26-53e3b041c6cb.tmp
24/07/20 16:06:41 INFO CheckpointFileManager: Renamed temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/.1.31b97e12-afed-42bc-8d26-53e3b041c6cb.tmp to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/1
24/07/20 16:06:41 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "4e439331-2ef5-4ccd-807b-7735ec4fe321",
  "runId" : "300a8ed0-c967-4502-864f-9b6811adc4ae",
  "name" : null,
  "timestamp" : "2024-07-20T16:06:40.146Z",
  "batchId" : 1,
  "numInputRows" : 10,
  "inputRowsPerSecond" : 769.2307692307693,
  "processedRowsPerSecond" : 5.652911249293386,
  "durationMs" : {
    "addBatch" : 1625,
    "commitOffsets" : 29,
    "getBatch" : 1,
    "latestOffset" : 2,
    "queryPlanning" : 63,
    "triggerExecution" : 1769,
    "walCommit" : 44
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[classifiedMessages]]",
    "startOffset" : {
      "classifiedMessages" : {
        "0" : 30
      }
    },
    "endOffset" : {
      "classifiedMessages" : {
        "0" : 40
      }
    },
    "latestOffset" : {
      "classifiedMessages" : {
        "0" : 40
      }
    },
    "numInputRows" : 10,
    "inputRowsPerSecond" : 769.2307692307693,
    "processedRowsPerSecond" : 5.652911249293386,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "FileSink[hdfs://hadoop-namenode:8020/user/spark/kafka-data]",
    "numOutputRows" : -1
  }
}
24/07/20 16:06:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:06:54 INFO BlockManagerInfo: Removed broadcast_1_piece0 on f29f783329e2:38415 in memory (size: 83.9 KiB, free: 434.4 MiB)
24/07/20 16:06:54 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.9:36951 in memory (size: 83.9 KiB, free: 434.4 MiB)
24/07/20 16:07:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:07:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:07:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:07:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:07:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:07:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:08:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:08:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:08:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:08:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:08:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:08:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:09:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:09:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:09:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:09:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:09:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:09:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:10:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:10:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:10:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:10:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:10:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:10:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:11:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:11:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:11:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:11:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:11:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:11:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:12:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:12:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:12:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:12:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:12:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:12:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 16:13:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
^CERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: reentrant call inside <_io.BufferedReader name=3>

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
24/07/20 16:13:11 INFO SparkContext: Invoking stop() from shutdown hook
24/07/20 16:13:11 INFO SparkContext: SparkContext is stopping with exitCode 0.
ERROR:root:Exception while sending command.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 511, in send_command
    answer = smart_decode(self.stream.readline()[:-1])
                          ^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/python/lib/python3.11/socket.py", line 706, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 381, in signal_handler
    self.cancelAllJobs()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/context.py", line 2446, in cancelAllJobs
    self._jsc.sc().cancelAllJobs()
    ^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o23.sc

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1038, in send_command
    response = connection.send_command(command)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 539, in send_command
    raise Py4JNetworkError(
py4j.protocol.Py4JNetworkError: Error while sending or receiving
Traceback (most recent call last):
  File "/opt/spark-apps/ingestion.py", line 43, in <module>
    query.awaitTermination()
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 334, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling o62.awaitTermination
24/07/20 16:13:11 INFO SparkUI: Stopped Spark web UI at http://f29f783329e2:4040
24/07/20 16:13:11 INFO StandaloneSchedulerBackend: Shutting down all executors
24/07/20 16:13:11 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
24/07/20 16:13:11 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
24/07/20 16:13:11 INFO MemoryStore: MemoryStore cleared
24/07/20 16:13:11 INFO BlockManager: BlockManager stopped
24/07/20 16:13:11 INFO BlockManagerMaster: BlockManagerMaster stopped
24/07/20 16:13:11 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
24/07/20 16:13:11 INFO SparkContext: Successfully stopped SparkContext
24/07/20 16:13:11 INFO ShutdownHookManager: Shutdown hook called
24/07/20 16:13:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-a83dbccf-81a7-4684-9437-02ac3a0f1161
24/07/20 16:13:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-c343c4c1-c761-4a36-9bcf-923a1c1be07e
24/07/20 16:13:11 INFO ShutdownHookManager: Deleting directory /tmp/spark-c343c4c1-c761-4a36-9bcf-923a1c1be07e/pyspark-cb019bb8-eec7-47ee-abf9-37a2c028e2ae
