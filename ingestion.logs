:: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /root/.ivy2/cache
The jars for the packages stored in: /root/.ivy2/jars
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-1024639f-a536-4fe7-a016-13f7d631c3b9;1.0
	confs: [default]
	found org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 in central
	found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 in central
	found org.apache.kafka#kafka-clients;2.8.1 in central
	found org.lz4#lz4-java;1.8.0 in central
	found org.xerial.snappy#snappy-java;1.1.8.4 in central
	found org.slf4j#slf4j-api;1.7.32 in central
	found org.apache.hadoop#hadoop-client-runtime;3.3.2 in central
	found org.spark-project.spark#unused;1.0.0 in central
	found org.apache.hadoop#hadoop-client-api;3.3.2 in central
	found commons-logging#commons-logging;1.1.3 in central
	found com.google.code.findbugs#jsr305;3.0.0 in central
	found org.apache.commons#commons-pool2;2.11.1 in central
:: resolution report :: resolve 1316ms :: artifacts dl 57ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]
	org.apache.kafka#kafka-clients;2.8.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.0 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.slf4j#slf4j-api;1.7.32 from central in [default]
	org.spark-project.spark#unused;1.0.0 from central in [default]
	org.xerial.snappy#snappy-java;1.1.8.4 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-1024639f-a536-4fe7-a016-13f7d631c3b9
	confs: [default]
	0 artifacts copied, 12 already retrieved (0kB/17ms)
24/07/20 08:03:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
creating spark job
24/07/20 08:03:15 INFO SparkContext: Running Spark version 3.5.1
24/07/20 08:03:15 INFO SparkContext: OS info Linux, 6.1.85+, amd64
24/07/20 08:03:15 INFO SparkContext: Java version 17.0.11
24/07/20 08:03:15 INFO ResourceUtils: ==============================================================
24/07/20 08:03:15 INFO ResourceUtils: No custom resources configured for spark.driver.
24/07/20 08:03:15 INFO ResourceUtils: ==============================================================
24/07/20 08:03:15 INFO SparkContext: Submitted application: KafkaSparkConsumer
24/07/20 08:03:15 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
24/07/20 08:03:15 INFO ResourceProfile: Limiting resource is cpu
24/07/20 08:03:15 INFO ResourceProfileManager: Added ResourceProfile id: 0
24/07/20 08:03:15 INFO SecurityManager: Changing view acls to: root,spark
24/07/20 08:03:15 INFO SecurityManager: Changing modify acls to: root,spark
24/07/20 08:03:15 INFO SecurityManager: Changing view acls groups to: 
24/07/20 08:03:15 INFO SecurityManager: Changing modify acls groups to: 
24/07/20 08:03:15 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root, spark; groups with view permissions: EMPTY; users with modify permissions: root, spark; groups with modify permissions: EMPTY
24/07/20 08:03:16 INFO Utils: Successfully started service 'sparkDriver' on port 39787.
24/07/20 08:03:16 INFO SparkEnv: Registering MapOutputTracker
24/07/20 08:03:16 INFO SparkEnv: Registering BlockManagerMaster
24/07/20 08:03:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
24/07/20 08:03:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
24/07/20 08:03:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
24/07/20 08:03:16 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-decdfd86-cda3-463d-88aa-76b08b0789cc
24/07/20 08:03:16 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
24/07/20 08:03:16 INFO SparkEnv: Registering OutputCommitCoordinator
24/07/20 08:03:16 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
24/07/20 08:03:16 INFO Utils: Successfully started service 'SparkUI' on port 4040.
24/07/20 08:03:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at spark://a3fd1c5ed807:39787/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at spark://a3fd1c5ed807:39787/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at spark://a3fd1c5ed807:39787/jars/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://a3fd1c5ed807:39787/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://a3fd1c5ed807:39787/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://a3fd1c5ed807:39787/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at spark://a3fd1c5ed807:39787/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://a3fd1c5ed807:39787/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at spark://a3fd1c5ed807:39787/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at spark://a3fd1c5ed807:39787/jars/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at spark://a3fd1c5ed807:39787/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://a3fd1c5ed807:39787/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar at spark://a3fd1c5ed807:39787/files/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-9e0ba4af-f574-476c-817b-5f0f82e7c4de/userFiles-ca8aee3f-f1ea-45e8-87bf-e00279158d5d/org.apache.spark_spark-sql-kafka-0-10_2.12-3.3.0.jar
24/07/20 08:03:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar at spark://a3fd1c5ed807:39787/files/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar to /tmp/spark-9e0ba4af-f574-476c-817b-5f0f82e7c4de/userFiles-ca8aee3f-f1ea-45e8-87bf-e00279158d5d/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.3.0.jar
24/07/20 08:03:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar at spark://a3fd1c5ed807:39787/files/org.apache.kafka_kafka-clients-2.8.1.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.1.jar to /tmp/spark-9e0ba4af-f574-476c-817b-5f0f82e7c4de/userFiles-ca8aee3f-f1ea-45e8-87bf-e00279158d5d/org.apache.kafka_kafka-clients-2.8.1.jar
24/07/20 08:03:16 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://a3fd1c5ed807:39787/files/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-9e0ba4af-f574-476c-817b-5f0f82e7c4de/userFiles-ca8aee3f-f1ea-45e8-87bf-e00279158d5d/com.google.code.findbugs_jsr305-3.0.0.jar
24/07/20 08:03:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://a3fd1c5ed807:39787/files/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-9e0ba4af-f574-476c-817b-5f0f82e7c4de/userFiles-ca8aee3f-f1ea-45e8-87bf-e00279158d5d/org.apache.commons_commons-pool2-2.11.1.jar
24/07/20 08:03:16 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://a3fd1c5ed807:39787/files/org.spark-project.spark_unused-1.0.0.jar with timestamp 1721462595378
24/07/20 08:03:16 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-9e0ba4af-f574-476c-817b-5f0f82e7c4de/userFiles-ca8aee3f-f1ea-45e8-87bf-e00279158d5d/org.spark-project.spark_unused-1.0.0.jar
24/07/20 08:03:17 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar at spark://a3fd1c5ed807:39787/files/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar with timestamp 1721462595378
24/07/20 08:03:17 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar to /tmp/spark-9e0ba4af-f574-476c-817b-5f0f82e7c4de/userFiles-ca8aee3f-f1ea-45e8-87bf-e00279158d5d/org.apache.hadoop_hadoop-client-runtime-3.3.2.jar
24/07/20 08:03:17 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://a3fd1c5ed807:39787/files/org.lz4_lz4-java-1.8.0.jar with timestamp 1721462595378
24/07/20 08:03:17 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-9e0ba4af-f574-476c-817b-5f0f82e7c4de/userFiles-ca8aee3f-f1ea-45e8-87bf-e00279158d5d/org.lz4_lz4-java-1.8.0.jar
24/07/20 08:03:17 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at spark://a3fd1c5ed807:39787/files/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1721462595378
24/07/20 08:03:17 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-9e0ba4af-f574-476c-817b-5f0f82e7c4de/userFiles-ca8aee3f-f1ea-45e8-87bf-e00279158d5d/org.xerial.snappy_snappy-java-1.1.8.4.jar
24/07/20 08:03:17 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar at spark://a3fd1c5ed807:39787/files/org.slf4j_slf4j-api-1.7.32.jar with timestamp 1721462595378
24/07/20 08:03:17 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.32.jar to /tmp/spark-9e0ba4af-f574-476c-817b-5f0f82e7c4de/userFiles-ca8aee3f-f1ea-45e8-87bf-e00279158d5d/org.slf4j_slf4j-api-1.7.32.jar
24/07/20 08:03:17 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar at spark://a3fd1c5ed807:39787/files/org.apache.hadoop_hadoop-client-api-3.3.2.jar with timestamp 1721462595378
24/07/20 08:03:17 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.2.jar to /tmp/spark-9e0ba4af-f574-476c-817b-5f0f82e7c4de/userFiles-ca8aee3f-f1ea-45e8-87bf-e00279158d5d/org.apache.hadoop_hadoop-client-api-3.3.2.jar
24/07/20 08:03:17 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://a3fd1c5ed807:39787/files/commons-logging_commons-logging-1.1.3.jar with timestamp 1721462595378
24/07/20 08:03:17 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-9e0ba4af-f574-476c-817b-5f0f82e7c4de/userFiles-ca8aee3f-f1ea-45e8-87bf-e00279158d5d/commons-logging_commons-logging-1.1.3.jar
24/07/20 08:03:17 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
24/07/20 08:03:17 INFO TransportClientFactory: Successfully created connection to spark-master/172.18.0.6:7077 after 62 ms (0 ms spent in bootstraps)
24/07/20 08:03:17 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20240720080317-0004
24/07/20 08:03:17 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20240720080317-0004/0 on worker-20240720071755-172.18.0.8-46549 (172.18.0.8:46549) with 1 core(s)
24/07/20 08:03:17 INFO StandaloneSchedulerBackend: Granted executor ID app-20240720080317-0004/0 on hostPort 172.18.0.8:46549 with 1 core(s), 1024.0 MiB RAM
24/07/20 08:03:17 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37001.
24/07/20 08:03:17 INFO NettyBlockTransferService: Server created on a3fd1c5ed807:37001
24/07/20 08:03:17 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
24/07/20 08:03:17 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a3fd1c5ed807, 37001, None)
24/07/20 08:03:17 INFO BlockManagerMasterEndpoint: Registering block manager a3fd1c5ed807:37001 with 434.4 MiB RAM, BlockManagerId(driver, a3fd1c5ed807, 37001, None)
24/07/20 08:03:17 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a3fd1c5ed807, 37001, None)
24/07/20 08:03:17 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a3fd1c5ed807, 37001, None)
24/07/20 08:03:17 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20240720080317-0004/0 is now RUNNING
24/07/20 08:03:18 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
after creating spark job
24/07/20 08:03:19 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
24/07/20 08:03:19 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
24/07/20 08:03:28 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
24/07/20 08:03:28 INFO ResolveWriteToStream: Checkpoint root hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka resolved to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka.
24/07/20 08:03:28 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
24/07/20 08:03:28 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.18.0.8:41858) with ID 0,  ResourceProfileId 0
24/07/20 08:03:28 INFO BlockManagerMasterEndpoint: Registering block manager 172.18.0.8:46323 with 434.4 MiB RAM, BlockManagerId(0, 172.18.0.8, 46323, None)
24/07/20 08:03:29 INFO MicroBatchExecution: Starting [id = c82919d1-ff60-4830-be6a-bba3b4ba4f16, runId = 1f55f699-dcd7-416b-a46c-845652aaa958]. Use hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka to store the query checkpoint.
24/07/20 08:03:29 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@216d8b4d] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@4844d92d]
24/07/20 08:03:29 INFO OffsetSeqLog: BatchIds found from listing: 0, 1
24/07/20 08:03:29 INFO OffsetSeqLog: Getting latest batch 1
24/07/20 08:03:29 INFO OffsetSeqLog: BatchIds found from listing: 0, 1
24/07/20 08:03:29 INFO OffsetSeqLog: Getting latest batch 1
24/07/20 08:03:29 INFO CommitLog: BatchIds found from listing: 0, 1
24/07/20 08:03:29 INFO CommitLog: Getting latest batch 1
24/07/20 08:03:29 INFO MicroBatchExecution: Resuming at batch 2 with committed offsets {KafkaV2[Subscribe[classifiedMessages]]: {"classifiedMessages":{"0":20}}} and available offsets {KafkaV2[Subscribe[classifiedMessages]]: {"classifiedMessages":{"0":20}}}
24/07/20 08:03:29 INFO MicroBatchExecution: Stream started from {KafkaV2[Subscribe[classifiedMessages]]: {"classifiedMessages":{"0":20}}}
24/07/20 08:03:29 INFO AdminClientConfig: AdminClientConfig values: 
	bootstrap.servers = [kafka:9093]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS

24/07/20 08:03:30 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.
24/07/20 08:03:30 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.
24/07/20 08:03:30 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.
24/07/20 08:03:30 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.
24/07/20 08:03:30 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.
24/07/20 08:03:30 INFO AppInfoParser: Kafka version: 2.8.1
24/07/20 08:03:30 INFO AppInfoParser: Kafka commitId: 839b886f9b732b15
24/07/20 08:03:30 INFO AppInfoParser: Kafka startTimeMs: 1721462610192
24/07/20 08:03:31 INFO CheckpointFileManager: Writing atomically to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/2 using temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/.2.9465f06d-cfad-494a-a5f0-e818cda0d45c.tmp
24/07/20 08:03:32 INFO CheckpointFileManager: Renamed temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/.2.9465f06d-cfad-494a-a5f0-e818cda0d45c.tmp to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/2
24/07/20 08:03:32 INFO MicroBatchExecution: Committed offsets for batch 2. Metadata OffsetSeqMetadata(0,1721462611863,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/07/20 08:03:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 08:03:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 08:03:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 08:03:32 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 08:03:32 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1
24/07/20 08:03:33 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
24/07/20 08:03:33 INFO CodeGenerator: Code generated in 365.043944 ms
24/07/20 08:03:34 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/07/20 08:03:34 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/07/20 08:03:34 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)
24/07/20 08:03:34 INFO DAGScheduler: Parents of final stage: List()
24/07/20 08:03:34 INFO DAGScheduler: Missing parents: List()
24/07/20 08:03:34 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/07/20 08:03:34 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 232.7 KiB, free 434.2 MiB)
24/07/20 08:03:34 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 83.9 KiB, free 434.1 MiB)
24/07/20 08:03:34 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a3fd1c5ed807:37001 (size: 83.9 KiB, free: 434.3 MiB)
24/07/20 08:03:34 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
24/07/20 08:03:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/07/20 08:03:34 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
24/07/20 08:03:34 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 10918 bytes) 
24/07/20 08:03:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.18.0.8:46323 (size: 83.9 KiB, free: 434.3 MiB)
24/07/20 08:03:41 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 6765 ms on 172.18.0.8 (executor 0) (1/1)
24/07/20 08:03:41 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
24/07/20 08:03:41 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 7.148 s
24/07/20 08:03:41 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
24/07/20 08:03:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
24/07/20 08:03:41 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 7.286206 s
24/07/20 08:03:41 INFO FileFormatWriter: Start to commit write Job 13a18c48-f868-47b2-969d-fb5524f97d8b.
24/07/20 08:03:41 INFO FileStreamSinkLog: Set the compact interval to 10 [defaultCompactInterval: 10]
24/07/20 08:03:41 INFO CheckpointFileManager: Writing atomically to hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/2 using temp file hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/.2.05482627-a2e8-48f4-9c31-ec619df46bdd.tmp
24/07/20 08:03:41 INFO CheckpointFileManager: Renamed temp file hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/.2.05482627-a2e8-48f4-9c31-ec619df46bdd.tmp to hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/2
24/07/20 08:03:41 INFO ManifestFileCommitProtocol: Committed batch 2
24/07/20 08:03:41 INFO FileFormatWriter: Write Job 13a18c48-f868-47b2-969d-fb5524f97d8b committed. Elapsed time: 64 ms.
24/07/20 08:03:41 INFO FileFormatWriter: Finished processing stats for write job 13a18c48-f868-47b2-969d-fb5524f97d8b.
24/07/20 08:03:41 INFO CheckpointFileManager: Writing atomically to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/2 using temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/.2.3d24ed33-bbcd-4213-bf20-e4e05902f62b.tmp
24/07/20 08:03:41 INFO CheckpointFileManager: Renamed temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/.2.3d24ed33-bbcd-4213-bf20-e4e05902f62b.tmp to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/2
24/07/20 08:03:41 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "c82919d1-ff60-4830-be6a-bba3b4ba4f16",
  "runId" : "1f55f699-dcd7-416b-a46c-845652aaa958",
  "name" : null,
  "timestamp" : "2024-07-20T08:03:29.193Z",
  "batchId" : 2,
  "numInputRows" : 10,
  "inputRowsPerSecond" : 0.0,
  "processedRowsPerSecond" : 0.8134710811030669,
  "durationMs" : {
    "addBatch" : 8663,
    "commitOffsets" : 61,
    "getBatch" : 4,
    "latestOffset" : 2276,
    "queryPlanning" : 692,
    "triggerExecution" : 12275,
    "walCommit" : 182
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[classifiedMessages]]",
    "startOffset" : {
      "classifiedMessages" : {
        "0" : 20
      }
    },
    "endOffset" : {
      "classifiedMessages" : {
        "0" : 30
      }
    },
    "latestOffset" : {
      "classifiedMessages" : {
        "0" : 30
      }
    },
    "numInputRows" : 10,
    "inputRowsPerSecond" : 0.0,
    "processedRowsPerSecond" : 0.8134710811030669,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "FileSink[hdfs://hadoop-namenode:8020/user/spark/kafka-data]",
    "numOutputRows" : -1
  }
}
24/07/20 08:03:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.18.0.8:46323 in memory (size: 83.9 KiB, free: 434.4 MiB)
24/07/20 08:03:46 INFO BlockManagerInfo: Removed broadcast_0_piece0 on a3fd1c5ed807:37001 in memory (size: 83.9 KiB, free: 434.4 MiB)
24/07/20 08:03:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:04:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:04:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:04:21 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:04:31 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:04:41 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:04:51 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:05:01 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:05:11 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:05:11 INFO CheckpointFileManager: Writing atomically to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/3 using temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/.3.b95a093a-ed37-41fe-acb5-baa1d2fca345.tmp
24/07/20 08:05:11 INFO CheckpointFileManager: Renamed temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/.3.b95a093a-ed37-41fe-acb5-baa1d2fca345.tmp to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/offsets/3
24/07/20 08:05:11 INFO MicroBatchExecution: Committed offsets for batch 3. Metadata OffsetSeqMetadata(0,1721462711643,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
24/07/20 08:05:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 08:05:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 08:05:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 08:05:11 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
24/07/20 08:05:11 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 2
24/07/20 08:05:11 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter
24/07/20 08:05:11 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
24/07/20 08:05:11 INFO DAGScheduler: Got job 1 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions
24/07/20 08:05:11 INFO DAGScheduler: Final stage: ResultStage 1 (start at NativeMethodAccessorImpl.java:0)
24/07/20 08:05:11 INFO DAGScheduler: Parents of final stage: List()
24/07/20 08:05:11 INFO DAGScheduler: Missing parents: List()
24/07/20 08:05:11 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[7] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
24/07/20 08:05:11 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 232.7 KiB, free 434.2 MiB)
24/07/20 08:05:11 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 83.9 KiB, free 434.1 MiB)
24/07/20 08:05:11 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on a3fd1c5ed807:37001 (size: 83.9 KiB, free: 434.3 MiB)
24/07/20 08:05:11 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
24/07/20 08:05:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[7] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/07/20 08:05:11 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
24/07/20 08:05:11 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.18.0.8, executor 0, partition 0, PROCESS_LOCAL, 10918 bytes) 
24/07/20 08:05:12 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.18.0.8:46323 (size: 83.9 KiB, free: 434.3 MiB)
24/07/20 08:05:12 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 897 ms on 172.18.0.8 (executor 0) (1/1)
24/07/20 08:05:12 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
24/07/20 08:05:12 INFO DAGScheduler: ResultStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.964 s
24/07/20 08:05:12 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
24/07/20 08:05:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
24/07/20 08:05:12 INFO DAGScheduler: Job 1 finished: start at NativeMethodAccessorImpl.java:0, took 0.974636 s
24/07/20 08:05:12 INFO FileFormatWriter: Start to commit write Job 65173267-c16e-4dfe-abdf-ee2f1e784863.
24/07/20 08:05:12 INFO CheckpointFileManager: Writing atomically to hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/3 using temp file hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/.3.d9aabfab-2c3d-4b96-b8eb-e9b62e128ff0.tmp
24/07/20 08:05:12 INFO CheckpointFileManager: Renamed temp file hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/.3.d9aabfab-2c3d-4b96-b8eb-e9b62e128ff0.tmp to hdfs://hadoop-namenode:8020/user/spark/kafka-data/_spark_metadata/3
24/07/20 08:05:12 INFO ManifestFileCommitProtocol: Committed batch 3
24/07/20 08:05:12 INFO FileFormatWriter: Write Job 65173267-c16e-4dfe-abdf-ee2f1e784863 committed. Elapsed time: 31 ms.
24/07/20 08:05:12 INFO FileFormatWriter: Finished processing stats for write job 65173267-c16e-4dfe-abdf-ee2f1e784863.
24/07/20 08:05:12 INFO CheckpointFileManager: Writing atomically to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/3 using temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/.3.3efc6a57-2887-4d64-8e85-3fdc04176278.tmp
24/07/20 08:05:12 INFO CheckpointFileManager: Renamed temp file hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/.3.3efc6a57-2887-4d64-8e85-3fdc04176278.tmp to hdfs://hadoop-namenode:8020/user/spark/checkpoints/kafka/commits/3
24/07/20 08:05:12 INFO MicroBatchExecution: Streaming query made progress: {
  "id" : "c82919d1-ff60-4830-be6a-bba3b4ba4f16",
  "runId" : "1f55f699-dcd7-416b-a46c-845652aaa958",
  "name" : null,
  "timestamp" : "2024-07-20T08:05:11.641Z",
  "batchId" : 3,
  "numInputRows" : 10,
  "inputRowsPerSecond" : 526.3157894736843,
  "processedRowsPerSecond" : 8.064516129032258,
  "durationMs" : {
    "addBatch" : 1104,
    "commitOffsets" : 31,
    "getBatch" : 1,
    "latestOffset" : 2,
    "queryPlanning" : 45,
    "triggerExecution" : 1240,
    "walCommit" : 51
  },
  "stateOperators" : [ ],
  "sources" : [ {
    "description" : "KafkaV2[Subscribe[classifiedMessages]]",
    "startOffset" : {
      "classifiedMessages" : {
        "0" : 30
      }
    },
    "endOffset" : {
      "classifiedMessages" : {
        "0" : 40
      }
    },
    "latestOffset" : {
      "classifiedMessages" : {
        "0" : 40
      }
    },
    "numInputRows" : 10,
    "inputRowsPerSecond" : 526.3157894736843,
    "processedRowsPerSecond" : 8.064516129032258,
    "metrics" : {
      "avgOffsetsBehindLatest" : "0.0",
      "maxOffsetsBehindLatest" : "0",
      "minOffsetsBehindLatest" : "0"
    }
  } ],
  "sink" : {
    "description" : "FileSink[hdfs://hadoop-namenode:8020/user/spark/kafka-data]",
    "numOutputRows" : -1
  }
}
24/07/20 08:05:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:05:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on a3fd1c5ed807:37001 in memory (size: 83.9 KiB, free: 434.4 MiB)
24/07/20 08:05:24 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.18.0.8:46323 in memory (size: 83.9 KiB, free: 434.4 MiB)
24/07/20 08:05:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:05:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:05:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:06:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:06:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:06:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:06:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:06:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:06:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:07:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:07:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:07:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:07:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:07:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:07:52 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:08:02 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:08:12 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:08:22 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:08:32 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:08:42 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:08:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:09:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:09:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:09:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:09:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:09:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:09:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:10:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:10:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:10:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:10:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:10:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:10:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:11:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:11:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:11:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:11:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:11:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:11:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:12:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:12:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:12:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:12:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:12:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:12:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:13:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:13:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:13:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:13:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:13:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:13:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:14:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:14:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:14:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:14:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:14:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:14:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:15:03 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:15:13 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:15:23 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:15:33 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:15:43 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
24/07/20 08:15:53 INFO MicroBatchExecution: Streaming query has been idle and waiting for new data more than 10000 ms.
